{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16 as VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet201 as DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_image_rgb(x):\n",
    "    # define mean and std values\n",
    "    mean = [87.845, 96.965, 103.947]\n",
    "    std = [23.657, 16.474, 13.793]\n",
    "    # loop over image channels\n",
    "    for idx, mean_value in enumerate(mean):\n",
    "        x[..., idx] -= mean_value\n",
    "        x[..., idx] /= std[idx]\n",
    "    return x\n",
    "\n",
    "\n",
    "def preprocessing_image_ms(x):\n",
    "    # define mean and std values\n",
    "    mean = [1353.036, 1116.468, 1041.475, 945.344, 1198.498, 2004.878,\n",
    "            2376.699, 2303.738, 732.957, 12.092, 1818.820, 1116.271, 2602.579]\n",
    "    std = [65.479, 154.008, 187.997, 278.508, 228.122, 356.598, 456.035,\n",
    "           531.570, 98.947, 1.188, 378.993, 303.851, 503.181]\n",
    "    # loop over image channels\n",
    "    for idx, mean_value in enumerate(mean):\n",
    "        x[..., idx] -= mean_value\n",
    "        x[..., idx] /= std[idx]\n",
    "    return x\n",
    "\n",
    "\n",
    "def categorical_label_from_full_file_name(files, class_indices):\n",
    "    from keras.utils import to_categorical\n",
    "    import os\n",
    "    # file basename without extension\n",
    "    base_name = [os.path.splitext(os.path.basename(i))[0] for i in files]\n",
    "    # class label from filename\n",
    "    base_name = [i.split(\"_\")[0] for i in base_name]\n",
    "    # label to indices\n",
    "    image_class = [class_indices[i] for i in base_name]\n",
    "    # class indices to one-hot-label\n",
    "    return to_categorical(image_class, num_classes=len(class_indices))\n",
    "\n",
    "\n",
    "def simple_image_generator(files, class_indices, batch_size=32,\n",
    "                           rotation_range=0, horizontal_flip=False,\n",
    "                           vertical_flip=False):\n",
    "    from skimage.io import imread\n",
    "    from skimage.transform import rotate\n",
    "    import numpy as np\n",
    "    from random import sample, choice\n",
    "\n",
    "    while True:\n",
    "        # select batch_size number of samples without replacement\n",
    "        batch_files = sample(files, batch_size)\n",
    "        # get one_hot_label\n",
    "        batch_Y = categorical_label_from_full_file_name(batch_files,\n",
    "                                                        class_indices)\n",
    "        # array for images\n",
    "        batch_X = []\n",
    "        # loop over images of the current batch\n",
    "        for idx, input_path in enumerate(batch_files):\n",
    "            image = np.array(imread(input_path), dtype=float)\n",
    "            image = preprocessing_image_ms(image)\n",
    "            # process image\n",
    "            if horizontal_flip:\n",
    "                # randomly flip image up/down\n",
    "                if choice([True, False]):\n",
    "                    image = np.flipud(image)\n",
    "            if vertical_flip:\n",
    "                # randomly flip image left/right\n",
    "                if choice([True, False]):\n",
    "                    image = np.fliplr(image)\n",
    "            # rotate image by random angle between\n",
    "            # -rotation_range <= angle < rotation_range\n",
    "            if rotation_range is not 0:\n",
    "                angle = np.random.uniform(low=-abs(rotation_range),\n",
    "                                          high=abs(rotation_range))\n",
    "                image = rotate(image, angle, mode='reflect',\n",
    "                               order=1, preserve_range=True)\n",
    "            # put all together\n",
    "            batch_X += [image]\n",
    "        # convert lists to np.array\n",
    "        X = np.array(batch_X)\n",
    "        Y = np.array(batch_Y)\n",
    "\n",
    "        yield(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_split_datasets = \"~/RGB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_vgg = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_home = os.path.expanduser(\"E:/PAPER_RS_DEEPLEARNING/image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_split_datasets = path_to_split_datasets.replace(\"~\", path_to_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:/PAPER_RS_DEEPLEARNING/image/RGB'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = os.path.join(path_to_split_datasets, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:/PAPER_RS_DEEPLEARNING/image/RGB\\\\train'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_validation = os.path.join(path_to_split_datasets, \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:/PAPER_RS_DEEPLEARNING/image/RGB\\\\validation'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dirs = [sub_dir for sub_dir in os.listdir(path_to_train)\n",
    "            if os.path.isdir(os.path.join(path_to_train, sub_dir))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=(len(sub_dirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_vgg:\n",
    "    base_model = VGG(include_top = False,\n",
    "                     weights = 'imagenet',\n",
    "                    input_shape=(64,64,3))\n",
    "else:\n",
    "    base_model = DenseNet(include_top = False,\n",
    "                          weights = 'imagenet',\n",
    "                          input_shape=(64,64,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = base_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = GlobalAveragePooling2D()(top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'global_average_pooling2d_1/Mean:0' shape=(?, 512) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_vgg:\n",
    "    top_model = Dense(2048, activation='relu')(top_model)\n",
    "    top_model = Dense(2048, activation='relu')(top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Dense(num_classes, activation='softmax')(top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              1050624   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 45)                92205     \n",
      "=================================================================\n",
      "Total params: 20,053,869\n",
      "Trainable params: 20,053,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(fill_mode=\"reflect\",\n",
    "                                    rotation_range=45,\n",
    "                                    horizontal_flip=True,\n",
    "                                    vertical_flip=True,\n",
    "                                    preprocessing_function=preprocessing_image_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocessing_image_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18000 images belonging to 45 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(path_to_train,\n",
    "                                                    target_size=(64,64),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'overpass': 26, 'cloud': 9, 'meadow': 22, 'mountain': 25, 'stadium': 39, 'palace': 27, 'desert': 12, 'airport': 1, 'runway': 34, 'sparse_residential': 38, 'rectangular_farmland': 31, 'golf_course': 15, 'airplane': 0, 'railway': 29, 'river': 32, 'parking_lot': 28, 'harbor': 17, 'terrace': 42, 'snowberg': 37, 'chaparral': 6, 'roundabout': 33, 'intersection': 19, 'forest': 13, 'railway_station': 30, 'basketball_court': 3, 'storage_tank': 40, 'freeway': 14, 'ship': 36, 'wetland': 44, 'dense_residential': 11, 'baseball_diamond': 2, 'lake': 21, 'thermal_power_station': 43, 'sea_ice': 35, 'medium_residential': 23, 'bridge': 5, 'commercial_area': 10, 'circular_farmland': 8, 'island': 20, 'industrial_area': 18, 'ground_track_field': 16, 'tennis_court': 41, 'beach': 4, 'church': 7, 'mobile_home_park': 24}\n"
     ]
    }
   ],
   "source": [
    "print(class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13500 images belonging to 45 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = test_datagen.flow_from_directory(path_to_validation, \n",
    "                                                        target_size=(64, 64), \n",
    "                                                        batch_size=batch_size, \n",
    "                                                        class_mode='categorical') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_vgg:\n",
    "    file_name = \"vgg\"\n",
    "\n",
    "else:\n",
    "    file_name = \"dense\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(\"E:/PAPER_RS_DEEPLEARNING/image/RGB/\" + file_name + \n",
    "                               \"_rgb_transfer_init.\" + \n",
    "                              \"{epoch:02d}-{val_categorical_accuracy:.3f}.\" +\n",
    "                               \"hdf5\",\n",
    "                               monitor='val_categorical_accuracy',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True,\n",
    "                               mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopper = EarlyStopping(monitor='val_categorical_accuracy', \n",
    "                             patience=10, \n",
    "                             mode='max', \n",
    "                             restore_best_weights=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='./logs', write_graph=True, write_grads=True, \n",
    "                          write_images=True, update_freq='epoch') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "200/200 [==============================] - 119s 595ms/step - loss: 1.4756 - categorical_accuracy: 0.5675 - val_loss: 1.4065 - val_categorical_accuracy: 0.5859\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from 0.46449 to 0.58590, saving model to E:/PAPER_RS_DEEPLEARNING/image/RGB/vgg_rgb_transfer_init.01-0.586.hdf5\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, \n",
    "                               steps_per_epoch=200, \n",
    "                               epochs=1, \n",
    "                               callbacks=[checkpointer, earlystopper, \n",
    "                                          tensorboard], \n",
    "                               validation_data=validation_generator, \n",
    "                               validation_steps=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = len(history.history['loss'])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model.layers): \n",
    "    names.append([i, layer.name, layer.trainable]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 'input_1', False], [1, 'block1_conv1', False], [2, 'block1_conv2', False], [3, 'block1_pool', False], [4, 'block2_conv1', False], [5, 'block2_conv2', False], [6, 'block2_pool', False], [7, 'block3_conv1', False], [8, 'block3_conv2', False], [9, 'block3_conv3', False], [10, 'block3_pool', False], [11, 'block4_conv1', False], [12, 'block4_conv2', False], [13, 'block4_conv3', False], [14, 'block4_pool', False], [15, 'block5_conv1', False], [16, 'block5_conv2', False], [17, 'block5_conv3', False], [18, 'block5_pool', False], [19, 'global_average_pooling2d_1', True], [20, 'dense_1', True], [21, 'dense_2', True], [22, 'dense_3', True]]\n"
     ]
    }
   ],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_vgg: \n",
    "     for layer in model.layers[:4]: \n",
    "            layer.trainable = False \n",
    "     for layer in model.layers[4:]: \n",
    "         layer.trainable = True \n",
    "else: \n",
    "     for layer in model.layers[:7]: \n",
    "             layer.trainable = False \n",
    "     for layer in model.layers[7:]: \n",
    "             layer.trainable = True \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), \n",
    "               loss='categorical_crossentropy', \n",
    "               metrics=['categorical_accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_vgg: \n",
    "     file_name = \"vgg\" \n",
    "else: \n",
    "     file_name = \"dense\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(\"E:/PAPER_RS_DEEPLEARNING/image/RGB/\" + file_name + \n",
    "                                \"_rgb_transfer_final.\" + \n",
    "                                \"{epoch:02d}-{val_categorical_accuracy:.3f}\" + \n",
    "                                \".hdf5\", \n",
    "                                monitor='val_categorical_accuracy', \n",
    "                                verbose=1, \n",
    "                                save_best_only=True, \n",
    "                                mode='max') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopper = EarlyStopping(monitor='val_categorical_accuracy', \n",
    "                              patience=50, \n",
    "                              mode='max') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "100/100 [==============================] - 66s 664ms/step - loss: 1.1378 - categorical_accuracy: 0.6608 - val_loss: 1.2403 - val_categorical_accuracy: 0.6353\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from -inf to 0.63525, saving model to E:/PAPER_RS_DEEPLEARNING/image/RGB/vgg_rgb_transfer_final.03-0.635.hdf5\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 66s 658ms/step - loss: 1.0071 - categorical_accuracy: 0.6927 - val_loss: 1.1167 - val_categorical_accuracy: 0.6709\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.63525 to 0.67092, saving model to E:/PAPER_RS_DEEPLEARNING/image/RGB/vgg_rgb_transfer_final.04-0.671.hdf5\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 68s 679ms/step - loss: 0.9570 - categorical_accuracy: 0.7119 - val_loss: 1.1598 - val_categorical_accuracy: 0.6596\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.67092\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 63s 631ms/step - loss: 0.8887 - categorical_accuracy: 0.7297 - val_loss: 0.9989 - val_categorical_accuracy: 0.7033\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.67092 to 0.70333, saving model to E:/PAPER_RS_DEEPLEARNING/image/RGB/vgg_rgb_transfer_final.06-0.703.hdf5\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 64s 638ms/step - loss: 0.8397 - categorical_accuracy: 0.7431 - val_loss: 0.9745 - val_categorical_accuracy: 0.7105\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.70333 to 0.71049, saving model to E:/PAPER_RS_DEEPLEARNING/image/RGB/vgg_rgb_transfer_final.07-0.710.hdf5\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 63s 634ms/step - loss: 0.7974 - categorical_accuracy: 0.7495 - val_loss: 1.0107 - val_categorical_accuracy: 0.7003\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.71049\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 64s 636ms/step - loss: 0.7750 - categorical_accuracy: 0.7581 - val_loss: 1.0053 - val_categorical_accuracy: 0.7023\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.71049\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 64s 638ms/step - loss: 0.7398 - categorical_accuracy: 0.7714 - val_loss: 1.0111 - val_categorical_accuracy: 0.7058\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.71049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12762b76b00>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch=100, \n",
    "                    epochs=10, \n",
    "                    callbacks=[checkpointer, earlystopper, tensorboard], \n",
    "                    validation_data=validation_generator, \n",
    "                    validation_steps=500, \n",
    "                    initial_epoch=initial_epoch) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
